{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import lda\n",
    "import numpy as np\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import math\n",
    "import re\n",
    "import icu \n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = \"/home/vietphan/Downloads/fbcrawl/source.txt\"\n",
    "filename = \"/home/vietphan/Downloads/fbcrawl/corpus.txt\"\n",
    "rating_file = \"/home/vietphan/Downloads/fbcrawl/rating.txt\"\n",
    "source = \"/home/vietphan/Downloads/fbcrawl/source.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_name = []\n",
    "corpus = []\n",
    "page_id = []\n",
    "doc_count = 0\n",
    "rating = []\n",
    "with open(filename, 'r') as f:\n",
    "    line = f.readline()\n",
    "    while(line):\n",
    "        if(line[0]!=\"*\"):\n",
    "            corpus.append(line)\n",
    "            # doc_count = doc_count +1\n",
    "        line = f.readline()\n",
    "with open(rating_file, 'r') as f2:\n",
    "    line = f2.readline()\n",
    "    while(line):\n",
    "        if(line[0]==\"*\"):\n",
    "            page_id.append(doc_count)#-len(page_id))\n",
    "        else:\n",
    "            rating.append(np.int64(line))\n",
    "            doc_count = doc_count +1\n",
    "        line = f2.readline()\n",
    "# print(page_id)\n",
    "\n",
    "with open(source, 'r') as f2:\n",
    "    line = f2.readline()\n",
    "    while(line):\n",
    "        if(line[0]==\"*\"):\n",
    "#             page_id.append(doc_count-len(page_id))\n",
    "            a = 6\n",
    "        else:\n",
    "            source_name.append(line)\n",
    "            doc_count = doc_count +1\n",
    "        line = f2.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "vocab = vectorizer.get_feature_names()\n",
    "# print(vocab)\n",
    "# for i in range(len(page_id)-1):\n",
    "#     sum = 0\n",
    "#     t_corpus = []\n",
    "#     for j in range(page_id[i],page_id[i+1]):\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "# for j in range(page_id[-1],len(rating)):\n",
    "#     sum = sum + rating[j]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvi import ViTokenizer, ViPosTagger\n",
    "from pyvi import ViUtils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(line,stop_words,stop_words_no_accent):\n",
    "    words = line.split()\n",
    "    mid = 0\n",
    "    line = \"\"\n",
    "    for count,word in enumerate(words):\n",
    "        word_no_accent = ViUtils.remove_accents(u\"\"+word)\n",
    "        left = 0\n",
    "        right = len(stop_words)-1\n",
    "        while(int((right-left)/2)>0):\n",
    "            mid = int((right+left)/2)\n",
    "            # print(word_no_accent,stop_words_no_accent[mid],(word_no_accent>stop_words_no_accent[mid]),left,right)\n",
    "            if(word_no_accent>stop_words_no_accent[mid]):\n",
    "                left = mid\n",
    "            elif(word_no_accent<stop_words_no_accent[mid]):\n",
    "                right = mid\n",
    "            elif((word_no_accent==stop_words_no_accent[mid])):\n",
    "                break\n",
    "        if(word_no_accent != stop_words_no_accent[mid]):\n",
    "            # print(word,stop_words[mid])\n",
    "            line = line+word+\" \"\n",
    "        else:\n",
    "            if(mid>=1 and mid<=(len(stop_words)-1)):\n",
    "                check = 0\n",
    "                for i in range(mid-1,mid+1):\n",
    "                    if(word==stop_words[i]):\n",
    "                        check = 1\n",
    "                        break\n",
    "                if(check == 0):\n",
    "                    line = line+word+\" \"\n",
    "            elif(mid <=1):\n",
    "                check = 0\n",
    "                for i in range(0,mid+1):\n",
    "                    if(word==stop_words[i]):\n",
    "                        check = 1\n",
    "                        break\n",
    "                if(check == 0):\n",
    "                    line = line+word+\" \"\n",
    "            elif(mid >= (len(stop_words)-4)):\n",
    "                check = 0\n",
    "                for i in range(mid - 5,mid):\n",
    "                    if(word==stop_words[i]):\n",
    "                        check = 1\n",
    "                        break\n",
    "                if(check == 0):\n",
    "                    line = line+word+\" \"\n",
    "        # else:\n",
    "        #     print(word)\n",
    "    return line\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_trash(doc, low_IDF, low_IDF_no_acc):\n",
    "    line = ''\n",
    "    words = doc.split()\n",
    "#     print(low_IDF)\n",
    "    if(len(low_IDF)!=0):\n",
    "        for low in low_IDF:\n",
    "            for word in words:\n",
    "                if(word == low):\n",
    "                    words.remove(word)\n",
    "    for word in words:\n",
    "        line = line + word + \" \"\n",
    "    return line\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91\n"
     ]
    }
   ],
   "source": [
    "print(len(page_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 0\n",
      "Processing 1\n",
      "Processing 2\n",
      "Processing 3\n",
      "Processing 4\n",
      "Processing 5\n",
      "Processing 6\n",
      "Processing 7\n",
      "Processing 8\n",
      "Processing 9\n",
      "Processing 10\n",
      "Processing 11\n",
      "Processing 12\n",
      "Processing 13\n",
      "Processing 14\n",
      "Processing 15\n",
      "Processing 16\n",
      "Processing 17\n",
      "Processing 18\n",
      "Processing 19\n",
      "Processing 20\n",
      "Processing 21\n",
      "Processing 22\n",
      "Processing 23\n",
      "Processing 24\n",
      "Processing 25\n",
      "Processing 26\n",
      "Processing 27\n",
      "Processing 28\n",
      "Processing 29\n",
      "Processing 30\n",
      "Processing 31\n",
      "Processing 32\n",
      "Processing 33\n",
      "Processing 34\n",
      "Processing 35\n",
      "Processing 36\n",
      "Processing 37\n",
      "Processing 38\n",
      "Processing 39\n",
      "Processing 40\n",
      "Processing 41\n",
      "Processing 42\n",
      "Processing 43\n",
      "Processing 44\n",
      "Processing 45\n",
      "Processing 46\n",
      "Processing 47\n",
      "Processing 48\n",
      "Processing 49\n",
      "Processing 50\n",
      "Processing 51\n",
      "Processing 52\n",
      "Processing 53\n",
      "Processing 54\n",
      "Processing 55\n",
      "Processing 56\n",
      "Processing 57\n",
      "Processing 58\n",
      "Processing 59\n",
      "Processing 60\n",
      "Processing 61\n",
      "Processing 62\n",
      "Processing 63\n",
      "Processing 64\n",
      "Processing 65\n",
      "Processing 66\n",
      "Processing 67\n",
      "Processing 68\n",
      "Processing 69\n",
      "Processing 70\n",
      "Processing 71\n",
      "Processing 72\n",
      "Processing 73\n",
      "Processing 74\n",
      "Processing 75\n",
      "Processing 76\n",
      "Processing 77\n",
      "Processing 78\n",
      "Processing 79\n",
      "Processing 80\n",
      "Processing 81\n",
      "Processing 82\n",
      "Processing 83\n",
      "Processing 84\n",
      "Processing 85\n",
      "Processing 86\n",
      "Processing 87\n",
      "Processing 88\n",
      "Processing 89\n",
      "Processing 90\n"
     ]
    }
   ],
   "source": [
    "n = 80\n",
    "i=n\n",
    "\n",
    "for i in range(len(page_id)):\n",
    "    print(\"Processing \"+str(i))\n",
    "#     print(page_id[i+1])\n",
    "    k=i\n",
    "    t_corpus = []\n",
    "    low_IDF = []\n",
    "    low_IDF_no_acc = []\n",
    "    if(i<len(page_id)-1):\n",
    "        for j in range(page_id[i],page_id[i+1]):\n",
    "            t_corpus.append(corpus[j])\n",
    "    else:\n",
    "        for j in range(page_id[-1],len(rating)):\n",
    "            t_corpus.append(corpus[j])\n",
    "    if(len(t_corpus)>0):\n",
    "        vectorizer2 = CountVectorizer()\n",
    "        Y = vectorizer2.fit_transform(t_corpus)\n",
    "        vocab_page = vectorizer2.get_feature_names()\n",
    "        vector_page = Y.toarray()\n",
    "\n",
    "        for i, word in enumerate(vocab_page):\n",
    "                word = re.sub(r'\\#', '', word)\n",
    "                word = re.sub(r'\\>', '', word)\n",
    "                count_word_in_corpus = 0\n",
    "                ind = vocab_page.index(word) \n",
    "                for j in range(len(vector_page)):\n",
    "                    if(vector_page[j][ind]!=0):\n",
    "                        count_word_in_corpus+=1\n",
    "                IDF = math.log(len(vector_page)/count_word_in_corpus)\n",
    "        #         print(word+\" \"+str(IDF))\n",
    "                if(IDF<1):\n",
    "                    low_IDF.append(word)\n",
    "                    low_IDF_no_acc.append(ViUtils.remove_accents(u\"\"+word))\n",
    "\n",
    "        # test_corpus = []\n",
    "        if(k<len(page_id)-1):\n",
    "            for j in range(page_id[k],page_id[k+1]):\n",
    "                corpus[j] = remove_trash(corpus[j],low_IDF,low_IDF_no_acc)\n",
    "        else:\n",
    "            for j in range(page_id[-1],len(rating)):\n",
    "                corpus[j] = remove_trash(corpus[j],low_IDF,low_IDF_no_acc)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     test_corpus.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "done_processing = \"/home/vietphan/Downloads/fbcrawl/done_processing.txt\"\n",
    "with open(done_processing, 'w') as f:\n",
    "    for line in corpus:\n",
    "        if(len(line)!=0):\n",
    "            f.write(line)\n",
    "            f.write(\"\\n\")\n",
    "    \n",
    "# with open(done_processing) as infile, open(done_processing, 'w') as outfile:\n",
    "#     for line in infile:\n",
    "#         if not line.strip(): continue  # skip the empty line\n",
    "#         outfile.write(line)  # non-empty line. Write it to output\n",
    "\n",
    "# for j in range(page_id[k],page_id[k+1]):\n",
    "#     print(corpus[j])\n",
    "# t_corpus = []\n",
    "# for i in range(len(page_id)-1):\n",
    "#     print(page_id[i])\n",
    "#     for j in range(page_id[i],page_id[i+1]):\n",
    "#         t_corpus.append(corpus[j])\n",
    "# print(t_corpus[-1])\n",
    "# print(len(corpus))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_corpus' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-b9f15d7132a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_corpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'test_corpus' is not defined"
     ]
    }
   ],
   "source": [
    "print(test_corpus[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(test_corpus[50]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line = ViTokenizer.tokenize(u\"\"+test_corpus[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
