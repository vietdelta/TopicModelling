{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import lda\n",
    "import numpy as np\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import math\n",
    "import re\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"/home/vietphan/Downloads/fbcrawl/Data-Celeb-Nov/\"\n",
    "source = data_folder+\"model/\"+\"source.txt\"\n",
    "filename = data_folder+\"model/\"+\"corpus.txt\"\n",
    "rating_file = data_folder+\"model/\"+\"rating.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_name = []\n",
    "corpus = []\n",
    "page_id = []\n",
    "doc_count = 0\n",
    "rating = []\n",
    "with open(filename, 'r') as f:\n",
    "    line = f.readline()\n",
    "    while(line):\n",
    "        if(line[0]!=\"*\"):\n",
    "            corpus.append(line)\n",
    "            # doc_count = doc_count +1\n",
    "        line = f.readline()\n",
    "with open(rating_file, 'r') as f2:\n",
    "    line = f2.readline()\n",
    "    while(line):\n",
    "        if(line[0]==\"*\"):\n",
    "            page_id.append(doc_count)#-len(page_id))\n",
    "        else:\n",
    "            rating.append(np.int64(line))\n",
    "            doc_count = doc_count +1\n",
    "        line = f2.readline()\n",
    "# print(page_id)\n",
    "\n",
    "with open(source, 'r') as f2:\n",
    "    line = f2.readline()\n",
    "    while(line):\n",
    "        if(line[0]==\"*\"):\n",
    "#             page_id.append(doc_count-len(page_id))\n",
    "            a = 6\n",
    "        else:\n",
    "            source_name.append(line)\n",
    "            doc_count = doc_count +1\n",
    "        line = f2.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "vocab = vectorizer.get_feature_names()\n",
    "# print(vocab)\n",
    "# for i in range(len(page_id)-1):\n",
    "#     sum = 0\n",
    "#     t_corpus = []\n",
    "#     for j in range(page_id[i],page_id[i+1]):\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "# for j in range(page_id[-1],len(rating)):\n",
    "#     sum = sum + rating[j]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvi import ViTokenizer, ViPosTagger\n",
    "from pyvi import ViUtils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(line,stop_words,stop_words_no_accent):\n",
    "    words = line.split()\n",
    "    mid = 0\n",
    "    line = \"\"\n",
    "    for count,word in enumerate(words):\n",
    "        word_no_accent = ViUtils.remove_accents(u\"\"+word)\n",
    "        left = 0\n",
    "        right = len(stop_words)-1\n",
    "        while(int((right-left)/2)>0):\n",
    "            mid = int((right+left)/2)\n",
    "            # print(word_no_accent,stop_words_no_accent[mid],(word_no_accent>stop_words_no_accent[mid]),left,right)\n",
    "            if(word_no_accent>stop_words_no_accent[mid]):\n",
    "                left = mid\n",
    "            elif(word_no_accent<stop_words_no_accent[mid]):\n",
    "                right = mid\n",
    "            elif((word_no_accent==stop_words_no_accent[mid])):\n",
    "                break\n",
    "        if(word_no_accent != stop_words_no_accent[mid]):\n",
    "            # print(word,stop_words[mid])\n",
    "            line = line+word+\" \"\n",
    "        else:\n",
    "            if(mid>=1 and mid<=(len(stop_words)-1)):\n",
    "                check = 0\n",
    "                for i in range(mid-1,mid+1):\n",
    "                    if(word==stop_words[i]):\n",
    "                        check = 1\n",
    "                        break\n",
    "                if(check == 0):\n",
    "                    line = line+word+\" \"\n",
    "            elif(mid <=1):\n",
    "                check = 0\n",
    "                for i in range(0,mid+1):\n",
    "                    if(word==stop_words[i]):\n",
    "                        check = 1\n",
    "                        break\n",
    "                if(check == 0):\n",
    "                    line = line+word+\" \"\n",
    "            elif(mid >= (len(stop_words)-4)):\n",
    "                check = 0\n",
    "                for i in range(mid - 5,mid):\n",
    "                    if(word==stop_words[i]):\n",
    "                        check = 1\n",
    "                        break\n",
    "                if(check == 0):\n",
    "                    line = line+word+\" \"\n",
    "        # else:\n",
    "        #     print(word)\n",
    "    return line\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_trash(doc, low_IDF, low_IDF_no_acc):\n",
    "    line = ''\n",
    "    words = doc.split()\n",
    "#     print(low_IDF)\n",
    "    if(len(low_IDF)!=0):\n",
    "        for low in low_IDF:\n",
    "            for word in words:\n",
    "                if(word == low):\n",
    "                    words.remove(word)\n",
    "    for word in words:\n",
    "        line = line + word + \" \"\n",
    "    return line\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "0\n"
    }
   ],
   "source": [
    "print(len(page_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 80\n",
    "i=n\n",
    "\n",
    "for i in range(len(page_id)):\n",
    "    print(\"Processing \"+str(i))\n",
    "#     print(page_id[i+1])\n",
    "    k=i\n",
    "    t_corpus = []\n",
    "    low_IDF = []\n",
    "    low_IDF_no_acc = []\n",
    "    if(i<len(page_id)-1):\n",
    "        for j in range(page_id[i],page_id[i+1]):\n",
    "            t_corpus.append(corpus[j])\n",
    "    else:\n",
    "        for j in range(page_id[-1],len(rating)):\n",
    "            t_corpus.append(corpus[j])\n",
    "    if(len(t_corpus)>0):\n",
    "        vectorizer2 = CountVectorizer()\n",
    "        Y = vectorizer2.fit_transform(t_corpus)\n",
    "        vocab_page = vectorizer2.get_feature_names()\n",
    "        vector_page = Y.toarray()\n",
    "\n",
    "        for i, word in enumerate(vocab_page):\n",
    "                word = re.sub(r'\\#', '', word)\n",
    "                word = re.sub(r'\\>', '', word)\n",
    "                count_word_in_corpus = 0\n",
    "                ind = vocab_page.index(word) \n",
    "                for j in range(len(vector_page)):\n",
    "                    if(vector_page[j][ind]!=0):\n",
    "                        count_word_in_corpus+=1\n",
    "                IDF = math.log(len(vector_page)/count_word_in_corpus)\n",
    "        #         print(word+\" \"+str(IDF))\n",
    "                if(IDF<1):\n",
    "                    low_IDF.append(word)\n",
    "                    low_IDF_no_acc.append(ViUtils.remove_accents(u\"\"+word))\n",
    "\n",
    "        # test_corpus = []\n",
    "        if(k<len(page_id)-1):\n",
    "            for j in range(page_id[k],page_id[k+1]):\n",
    "                corpus[j] = remove_trash(corpus[j],low_IDF,low_IDF_no_acc)\n",
    "        else:\n",
    "            for j in range(page_id[-1],len(rating)):\n",
    "                corpus[j] = remove_trash(corpus[j],low_IDF,low_IDF_no_acc)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     test_corpus.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "done_processing = data_folder+\"model/\"+\"done_processing.txt\"\n",
    "with open(done_processing, 'w') as f:\n",
    "    for line in corpus:\n",
    "        if(len(line)!=0):\n",
    "            f.write(line)\n",
    "            f.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}